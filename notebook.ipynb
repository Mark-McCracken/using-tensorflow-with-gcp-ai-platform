{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache-beam[gcp]==2.16.0\n",
      "  Downloading apache_beam-2.16.0-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (2019.3)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (1.4.1)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (2.5.8)\n",
      "Requirement already satisfied: avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\" in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (1.9.2.1)\n",
      "Requirement already satisfied: grpcio<2,>=1.12.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (1.28.1)\n",
      "Requirement already satisfied: future<1.0.0,>=0.16.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (0.18.2)\n",
      "Requirement already satisfied: protobuf<4,>=3.5.0.post1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (3.11.4)\n",
      "Collecting pyyaml<4.0.0,>=3.12\n",
      "  Downloading PyYAML-3.13.tar.gz (270 kB)\n",
      "\u001b[K     |████████████████████████████████| 270 kB 55.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (1.7)\n",
      "Collecting mock<3.0.0,>=1.0.1\n",
      "  Downloading mock-2.0.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 6.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (3.10.1)\n",
      "Collecting httplib2<=0.12.0,>=0.8\n",
      "  Downloading httplib2-0.12.0.tar.gz (218 kB)\n",
      "\u001b[K     |████████████████████████████████| 218 kB 63.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow<0.15.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\"\n",
      "  Downloading pyarrow-0.14.1-cp37-cp37m-manylinux2010_x86_64.whl (58.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 58.1 MB 5.9 kB/s  eta 0:00:01     |██████████████████▉             | 34.1 MB 17.5 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: oauth2client<4,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (3.0.0)\n",
      "Requirement already satisfied: fastavro<0.22,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (0.21.24)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (2.8.1)\n",
      "Requirement already satisfied: dill<0.3.1,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (0.3.0)\n",
      "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (3.1.1)\n",
      "Collecting google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"\n",
      "  Downloading google_cloud_datastore-1.7.4-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (1.3.0)\n",
      "Collecting google-cloud-bigquery<1.18.0,>=1.6.0; extra == \"gcp\"\n",
      "  Downloading google_cloud_bigquery-1.17.1-py2.py3-none-any.whl (142 kB)\n",
      "\u001b[K     |████████████████████████████████| 142 kB 65.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"\n",
      "  Downloading google_cloud_bigtable-1.0.0-py2.py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 55.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-pubsub<1.1.0,>=0.39.0; extra == \"gcp\"\n",
      "  Downloading google_cloud_pubsub-1.0.2-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 57.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-apitools<0.5.29,>=0.5.28; extra == \"gcp\" in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (0.5.28)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/conda/lib/python3.7/site-packages (from pydot<2,>=1.2.0->apache-beam[gcp]==2.16.0) (2.4.7)\n",
      "Requirement already satisfied: requests>=2.7.0 in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (2.23.0)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (0.6.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf<4,>=3.5.0.post1->apache-beam[gcp]==2.16.0) (46.1.3.post20200325)\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.16.0) (5.4.5)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.7/site-packages (from pyarrow<0.15.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\"->apache-beam[gcp]==2.16.0) (1.18.3)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.16.0) (4.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.16.0) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.16.0) (0.2.8)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (1.17.0)\n",
      "Collecting google-resumable-media<0.5.0dev,>=0.3.1\n",
      "  Downloading google_resumable_media-0.4.1-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (0.12.3)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (0.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (3.0.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.6.0->google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (1.51.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.6.0->google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (1.14.0)\n",
      "Requirement already satisfied: monotonic>=0.1 in /opt/conda/lib/python3.7/site-packages (from fasteners>=0.14->google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (1.5)\n",
      "Building wheels for collected packages: pyyaml, httplib2\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-3.13-cp37-cp37m-linux_x86_64.whl size=43086 sha256=41f1a5bab3eabad702cf3a74af20e65672ed0c256ae5f7b0658b12f46c7268c0\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/95/cd/14/899edaa9cdb9a65aa7224539f6e0ad488e9a7b202bb48f6ae6\n",
      "  Building wheel for httplib2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for httplib2: filename=httplib2-0.12.0-py3-none-any.whl size=93464 sha256=a33fe052412644b9d65b16891dba4acd7b64e064be3a167d0c1800e46603182d\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/0d/e7/b6/0dd30343ceca921cfbd91f355041bd9c69e0f40b49f25b7b8a\n",
      "Successfully built pyyaml httplib2\n",
      "\u001b[31mERROR: tfx 0.21.2 has requirement apache-beam[gcp]<2.18,>=2.17, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx 0.21.2 has requirement pyarrow<0.16,>=0.15, but you'll have pyarrow 0.14.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx 0.21.2 has requirement pyyaml<6,>=5, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx-bsl 0.21.4 has requirement apache-beam[gcp]<3,>=2.17, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx-bsl 0.21.4 has requirement pyarrow<0.16.0,>=0.15.0, but you'll have pyarrow 0.14.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-transform 0.21.2 has requirement apache-beam[gcp]<3,>=2.17, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-model-analysis 0.21.6 has requirement apache-beam[gcp]<3,>=2.17, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-model-analysis 0.21.6 has requirement pyarrow<1,>=0.15, but you'll have pyarrow 0.14.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement apache-beam[gcp]<3,>=2.17, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement pandas<1,>=0.24, but you'll have pandas 1.0.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement pyarrow>=0.15, but you'll have pyarrow 0.14.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement scikit-learn<0.22,>=0.18, but you'll have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-cloud-storage 1.27.0 has requirement google-resumable-media<0.6dev,>=0.5.0, but you'll have google-resumable-media 0.4.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pyyaml, mock, httplib2, pyarrow, google-cloud-datastore, google-resumable-media, google-cloud-bigquery, google-cloud-bigtable, google-cloud-pubsub, apache-beam\n",
      "\u001b[33m  WARNING: The script plasma_store is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed apache-beam-2.16.0 google-cloud-bigquery-1.17.1 google-cloud-bigtable-1.0.0 google-cloud-datastore-1.7.4 google-cloud-pubsub-1.0.2 google-resumable-media-0.4.1 httplib2-0.12.0 mock-2.0.0 pyarrow-0.14.1 pyyaml-3.13\n",
      "Collecting tensorflow-transform==0.15.0\n",
      "  Downloading tensorflow-transform-0.15.0.tar.gz (222 kB)\n",
      "\u001b[K     |████████████████████████████████| 222 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py<0.9,>=0.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==0.15.0) (0.8.1)\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.16 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow-transform==0.15.0) (2.16.0)\n",
      "Requirement already satisfied: numpy<2,>=1.16 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==0.15.0) (1.18.3)\n",
      "Requirement already satisfied: protobuf<4,>=3.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==0.15.0) (3.11.4)\n",
      "Requirement already satisfied: pydot<2,>=1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==0.15.0) (1.4.1)\n",
      "Requirement already satisfied: six<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==0.15.0) (1.14.0)\n",
      "Collecting tensorflow-metadata<0.16,>=0.15\n",
      "  Downloading tensorflow_metadata-0.15.2-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: tensorflow<2.2,>=1.15 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==0.15.0) (2.1.0)\n",
      "Collecting tfx-bsl<0.16,>=0.15\n",
      "  Downloading tfx_bsl-0.15.3-cp37-cp37m-manylinux2010_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyarrow<0.15.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\" in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (0.14.1)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (3.10.1)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (2.8.1)\n",
      "Requirement already satisfied: pyyaml<4.0.0,>=3.12 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (3.13)\n",
      "Requirement already satisfied: dill<0.3.1,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (0.3.0)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (2.5.8)\n",
      "Requirement already satisfied: httplib2<=0.12.0,>=0.8 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (0.12.0)\n",
      "Requirement already satisfied: grpcio<2,>=1.12.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (1.28.1)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (2019.3)\n",
      "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (2.0.0)\n",
      "Requirement already satisfied: fastavro<0.22,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (0.21.24)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (1.7)\n",
      "Requirement already satisfied: future<1.0.0,>=0.16.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (0.18.2)\n",
      "Requirement already satisfied: avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\" in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (1.9.2.1)\n",
      "Requirement already satisfied: oauth2client<4,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (3.0.0)\n",
      "Requirement already satisfied: google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\" in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (1.0.0)\n",
      "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (3.1.1)\n",
      "Requirement already satisfied: google-cloud-pubsub<1.1.0,>=0.39.0; extra == \"gcp\" in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (1.0.2)\n",
      "Requirement already satisfied: google-apitools<0.5.29,>=0.5.28; extra == \"gcp\" in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (0.5.28)\n",
      "Requirement already satisfied: google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\" in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (1.7.4)\n",
      "Requirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (1.3.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<1.18.0,>=1.6.0; extra == \"gcp\" in /home/jupyter/.local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (1.17.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf<4,>=3.7->tensorflow-transform==0.15.0) (46.1.3.post20200325)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/conda/lib/python3.7/site-packages (from pydot<2,>=1.2->tensorflow-transform==0.15.0) (2.4.7)\n",
      "Requirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata<0.16,>=0.15->tensorflow-transform==0.15.0) (1.51.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (0.8.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (1.0.8)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (2.1.0)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (2.1.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (0.34.2)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (1.4.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (1.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (0.2.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (1.12.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-serving-api<3,>=1.15 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl<0.16,>=0.15->tensorflow-transform==0.15.0) (2.1.0)\n",
      "Requirement already satisfied: psutil<6,>=5.6 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl<0.16,>=0.15->tensorflow-transform==0.15.0) (5.7.0)\n",
      "Requirement already satisfied: requests>=2.7.0 in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (2.23.0)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (0.6.2)\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (5.4.5)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (4.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (1.17.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (0.12.3)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (0.15)\n",
      "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery<1.18.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (0.4.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (2.10.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (3.2.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (1.14.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (2.9)\n",
      "Requirement already satisfied: monotonic>=0.1 in /opt/conda/lib/python3.7/site-packages (from fasteners>=0.14->google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.16->tensorflow-transform==0.15.0) (1.5)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=1.15->tensorflow-transform==0.15.0) (3.1.0)\n",
      "Building wheels for collected packages: tensorflow-transform\n",
      "  Building wheel for tensorflow-transform (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorflow-transform: filename=tensorflow_transform-0.15.0-py3-none-any.whl size=280591 sha256=0d1c7554c55a0563bf60954282466b54076824d510acd9d4c3b8cfe230565b55\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/25/9e/5a/3616db66925c4a6ff4fdf1666f0b1ff869247519683aec02cd\n",
      "Successfully built tensorflow-transform\n",
      "\u001b[31mERROR: tfx 0.21.2 has requirement apache-beam[gcp]<2.18,>=2.17, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx 0.21.2 has requirement pyarrow<0.16,>=0.15, but you'll have pyarrow 0.14.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx 0.21.2 has requirement pyyaml<6,>=5, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx 0.21.2 has requirement tensorflow-transform<0.22,>=0.21.2, but you'll have tensorflow-transform 0.15.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx 0.21.2 has requirement tfx-bsl<0.22,>=0.21.3, but you'll have tfx-bsl 0.15.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-model-analysis 0.21.6 has requirement apache-beam[gcp]<3,>=2.17, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-model-analysis 0.21.6 has requirement pyarrow<1,>=0.15, but you'll have pyarrow 0.14.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-model-analysis 0.21.6 has requirement tensorflow-metadata<0.22,>=0.21, but you'll have tensorflow-metadata 0.15.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-model-analysis 0.21.6 has requirement tfx-bsl<0.22,>=0.21.3, but you'll have tfx-bsl 0.15.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement apache-beam[gcp]<3,>=2.17, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement pandas<1,>=0.24, but you'll have pandas 1.0.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement pyarrow>=0.15, but you'll have pyarrow 0.14.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement scikit-learn<0.22,>=0.18, but you'll have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement tensorflow-metadata<0.22,>=0.21.1, but you'll have tensorflow-metadata 0.15.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement tensorflow-transform<0.22,>=0.21, but you'll have tensorflow-transform 0.15.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement tfx-bsl<0.22,>=0.21.3, but you'll have tfx-bsl 0.15.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorflow-metadata, tfx-bsl, tensorflow-transform\n",
      "Successfully installed tensorflow-metadata-0.15.2 tensorflow-transform-0.15.0 tfx-bsl-0.15.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --user apache-beam[gcp]==2.16.0\n",
    "!pip install --user tensorflow-transform==0.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-transform==0.15.0\n",
      "  Using cached tensorflow-transform-0.15.0.tar.gz (222 kB)\n",
      "  Saved ./tensorflow-transform-0.15.0.tar.gz\n",
      "Successfully downloaded tensorflow-transform\n"
     ]
    }
   ],
   "source": [
    "!pip download tensorflow-transform==0.15.0 --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-beam==2.16.0\n",
      "tensorflow==2.1.0\n",
      "tensorflow-data-validation==0.21.5\n",
      "tensorflow-datasets==2.0.0\n",
      "tensorflow-estimator==2.1.0\n",
      "tensorflow-hub==0.7.0\n",
      "tensorflow-io==0.11.0\n",
      "tensorflow-metadata==0.15.2\n",
      "tensorflow-model-analysis==0.21.6\n",
      "tensorflow-probability==0.9.0\n",
      "tensorflow-serving-api==2.1.0\n",
      "tensorflow-transform==0.15.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip freeze | grep -e 'flow\\|beam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0-dlenv_tfe\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'qwiklabs-gcp-03-cc939e8b47a1'\n",
    "project = 'qwiklabs-gcp-03-cc939e8b47a1'\n",
    "region = 'us-west1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = bucket\n",
    "os.environ['PROJECT'] = project\n",
    "os.environ['REGION'] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "    gstuil mv -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "def create_query(phase, EVERY_N):\n",
    "    farm_fingerprint_value = 4 if EVERY_N is None else EVERY_N\n",
    "    fingerprint_predicate = \"< 2\" if EVERY_N is None and phase < 2 else f\"= {phase}\" \n",
    "    return f\"\"\"\n",
    "        with daynames as (\n",
    "             select ['Sun', 'Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat'] AS daysofweek\n",
    "        )\n",
    "        select\n",
    "            (tolls_amount + fare_amount) AS fare_amount,\n",
    "            daysofweek[ORDINAL(EXTRACT(DAYOFWEEK FROM pickup_datetime))] AS dayofweek,\n",
    "            EXTRACT(HOUR FROM pickup_datetime) AS hourofday,\n",
    "            pickup_longitude AS pickuplon,\n",
    "            pickup_latitude AS pickuplat,\n",
    "            dropoff_longitude AS dropofflon,\n",
    "            dropoff_latitude AS dropofflat,\n",
    "            passenger_count AS passengers,\n",
    "            'notneeded' AS key\n",
    "        from\n",
    "            `nyc-tlc.yellow.trips`,\n",
    "            daynames\n",
    "        where trip_distance > 0\n",
    "        and fare_amount > 0\n",
    "        and abs(mod(farm_fingerprint(cast(pickup_datetime as string)), {farm_fingerprint_value})) {fingerprint_predicate}\n",
    "    \"\"\"\n",
    "\n",
    "query = create_query(2, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>pickuplon</th>\n",
       "      <th>pickuplat</th>\n",
       "      <th>dropofflon</th>\n",
       "      <th>dropofflat</th>\n",
       "      <th>passengers</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.974270</td>\n",
       "      <td>40.751047</td>\n",
       "      <td>-73.981585</td>\n",
       "      <td>40.748087</td>\n",
       "      <td>6</td>\n",
       "      <td>notneeded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.5</td>\n",
       "      <td>Mon</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.978730</td>\n",
       "      <td>40.724122</td>\n",
       "      <td>-73.982335</td>\n",
       "      <td>40.762377</td>\n",
       "      <td>5</td>\n",
       "      <td>notneeded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.5</td>\n",
       "      <td>Sun</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.971868</td>\n",
       "      <td>40.797323</td>\n",
       "      <td>-73.933123</td>\n",
       "      <td>40.856320</td>\n",
       "      <td>1</td>\n",
       "      <td>notneeded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.984160</td>\n",
       "      <td>40.729150</td>\n",
       "      <td>-73.984350</td>\n",
       "      <td>40.736320</td>\n",
       "      <td>1</td>\n",
       "      <td>notneeded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>0</td>\n",
       "      <td>-74.003830</td>\n",
       "      <td>40.722317</td>\n",
       "      <td>-74.002335</td>\n",
       "      <td>40.727867</td>\n",
       "      <td>1</td>\n",
       "      <td>notneeded</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fare_amount dayofweek  hourofday  pickuplon  pickuplat  dropofflon  \\\n",
       "0          5.0       Sun          0 -73.974270  40.751047  -73.981585   \n",
       "1         15.5       Mon          0 -73.978730  40.724122  -73.982335   \n",
       "2         16.5       Sun          0 -73.971868  40.797323  -73.933123   \n",
       "3          5.0       Sun          0 -73.984160  40.729150  -73.984350   \n",
       "4          5.0       Sun          0 -74.003830  40.722317  -74.002335   \n",
       "\n",
       "   dropofflat  passengers        key  \n",
       "0   40.748087           6  notneeded  \n",
       "1   40.762377           5  notneeded  \n",
       "2   40.856320           1  notneeded  \n",
       "3   40.736320           1  notneeded  \n",
       "4   40.727867           1  notneeded  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>pickuplon</th>\n",
       "      <th>pickuplat</th>\n",
       "      <th>dropofflon</th>\n",
       "      <th>dropofflat</th>\n",
       "      <th>passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11181.000000</td>\n",
       "      <td>11181.000000</td>\n",
       "      <td>11181.000000</td>\n",
       "      <td>11181.000000</td>\n",
       "      <td>11181.000000</td>\n",
       "      <td>11181.000000</td>\n",
       "      <td>11181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.242599</td>\n",
       "      <td>13.244075</td>\n",
       "      <td>-72.576852</td>\n",
       "      <td>39.973146</td>\n",
       "      <td>-72.748974</td>\n",
       "      <td>40.006091</td>\n",
       "      <td>1.722118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.447462</td>\n",
       "      <td>6.548354</td>\n",
       "      <td>10.133452</td>\n",
       "      <td>5.777329</td>\n",
       "      <td>12.981577</td>\n",
       "      <td>5.664887</td>\n",
       "      <td>1.351062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-78.133333</td>\n",
       "      <td>-73.991278</td>\n",
       "      <td>-751.400000</td>\n",
       "      <td>-73.977970</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-73.991849</td>\n",
       "      <td>40.734954</td>\n",
       "      <td>-73.991236</td>\n",
       "      <td>40.734008</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>-73.981824</td>\n",
       "      <td>40.752640</td>\n",
       "      <td>-73.980164</td>\n",
       "      <td>40.753427</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.500000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>-73.967418</td>\n",
       "      <td>40.766700</td>\n",
       "      <td>-73.964153</td>\n",
       "      <td>40.767832</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>143.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>40.806487</td>\n",
       "      <td>41.366138</td>\n",
       "      <td>40.785400</td>\n",
       "      <td>41.366138</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fare_amount     hourofday     pickuplon     pickuplat    dropofflon  \\\n",
       "count  11181.000000  11181.000000  11181.000000  11181.000000  11181.000000   \n",
       "mean      11.242599     13.244075    -72.576852     39.973146    -72.748974   \n",
       "std        9.447462      6.548354     10.133452      5.777329     12.981577   \n",
       "min        2.500000      0.000000    -78.133333    -73.991278   -751.400000   \n",
       "25%        6.000000      9.000000    -73.991849     40.734954    -73.991236   \n",
       "50%        8.500000     14.000000    -73.981824     40.752640    -73.980164   \n",
       "75%       12.500000     19.000000    -73.967418     40.766700    -73.964153   \n",
       "max      143.000000     23.000000     40.806487     41.366138     40.785400   \n",
       "\n",
       "         dropofflat    passengers  \n",
       "count  11181.000000  11181.000000  \n",
       "mean      40.006091      1.722118  \n",
       "std        5.664887      1.351062  \n",
       "min      -73.977970      0.000000  \n",
       "25%       40.734008      1.000000  \n",
       "50%       40.753427      1.000000  \n",
       "75%       40.767832      2.000000  \n",
       "max       41.366138      6.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = bigquery.Client().query(query).to_dataframe()\n",
    "display(df_valid.head())\n",
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ML dataset using tf.transform and dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching dataflow job preprocess-taxi-features-200503-110527\n",
      "WARNING:tensorflow:From <ipython-input-9-f39e579c847f>:108: ColumnSchema (from tensorflow_transform.tf_metadata.dataset_schema) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "ColumnSchema is a deprecated, use from_feature_spec to create a `Schema`\n",
      "WARNING:tensorflow:From <ipython-input-9-f39e579c847f>:119: Schema (from tensorflow_transform.tf_metadata.dataset_schema) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Schema is a deprecated, use schema_utils.schema_from_feature_spec to create a `Schema`\n",
      "WARNING:tensorflow:Tensorflow version (2.1.0-dlenv_tfe) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.1.0-dlenv_tfe) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.1.0-dlenv_tfe) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.1.0-dlenv_tfe) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dayofweek': <tf.Tensor 'inputs/inputs/dayofweek_copy:0' shape=(None,) dtype=string>, 'dropofflat': <tf.Tensor 'inputs/inputs/dropofflat_copy:0' shape=(None,) dtype=float32>, 'dropofflon': <tf.Tensor 'inputs/inputs/dropofflon_copy:0' shape=(None,) dtype=float32>, 'fare_amount': <tf.Tensor 'inputs/inputs/F_fare_amount_copy:0' shape=(None,) dtype=float32>, 'hourofday': <tf.Tensor 'inputs/inputs/hourofday_copy:0' shape=(None,) dtype=int64>, 'key': <tf.Tensor 'inputs/inputs/key_copy:0' shape=(None,) dtype=string>, 'passengers': <tf.Tensor 'inputs/inputs/passengers_copy:0' shape=(None,) dtype=int64>, 'pickuplat': <tf.Tensor 'inputs/inputs/pickuplat_copy:0' shape=(None,) dtype=float32>, 'pickuplon': <tf.Tensor 'inputs/inputs/pickuplon_copy:0' shape=(None,) dtype=float32>}\n",
      "WARNING:tensorflow:From <ipython-input-9-f39e579c847f>:48: string_to_int (from tensorflow_transform.mappers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tft.compute_and_apply_vocabulary()` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-f39e579c847f>:48: string_to_int (from tensorflow_transform.mappers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tft.compute_and_apply_vocabulary()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/tmp/tftransform_tmp/1397e5573e8e4a208c88c354697db6ae/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/tmp/tftransform_tmp/1397e5573e8e4a208c88c354697db6ae/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/tmp/tftransform_tmp/c9320c2720524ca8885c9b0d48d1dfab/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/tmp/tftransform_tmp/c9320c2720524ca8885c9b0d48d1dfab/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.1.0-dlenv_tfe) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.1.0-dlenv_tfe) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.1.0-dlenv_tfe) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.1.0-dlenv_tfe) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_metadata as tfmd\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "\n",
    "def is_valid(inputs):\n",
    "    \"\"\"Args: inputs, dict, dictionary of TableRow data from BQ\"\"\"\n",
    "    try:\n",
    "        pickup_longitude = inputs[\"pickuplon\"]\n",
    "        pickup_latitude = inputs[\"pickuplat\"]\n",
    "        dropoff_longitude = inputs[\"dropofflon\"]\n",
    "        dropoff_latitude = inputs[\"dropofflat\"]\n",
    "        passengers = inputs[\"passengers\"]\n",
    "        hour = inputs[\"hourofday\"]\n",
    "        day = inputs[\"dayofweek\"]\n",
    "        fare = inputs[\"fare_amount\"]\n",
    "        return (fare >= 2.5\n",
    "                and pickup_longitude > -78 and pickup_longitude < -70\n",
    "                and pickup_latitude > 37 and pickup_longitude < 45\n",
    "                and dropoff_longitude > -78 and dropoff_longitude < -70\n",
    "                and dropoff_latitude > 37 and dropoff_longitude < 45\n",
    "                and passengers > 0\n",
    "               )\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def preprocess_tft(inputs):\n",
    "    \"\"\"add engineered features with tf transform.\n",
    "    Args: dict, dictionary of TableRow data from BQ\n",
    "    returns dict of preprocessed data after scaling and feature engineering\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    print(inputs)\n",
    "    pickup_longitude = inputs[\"pickuplon\"]\n",
    "    pickup_latitude = inputs[\"pickuplat\"]\n",
    "    dropoff_longitude = inputs[\"dropofflon\"]\n",
    "    dropoff_latitude = inputs[\"dropofflat\"]\n",
    "    passengers = inputs[\"passengers\"]\n",
    "    hour = inputs[\"hourofday\"]\n",
    "    day = inputs[\"dayofweek\"]\n",
    "    fare = inputs[\"fare_amount\"]\n",
    "    \n",
    "    result = {}\n",
    "    result['fare'] = tf.identity(fare)\n",
    "    result['day'] = tft.string_to_int(day)\n",
    "    result['hour'] = tf.identity(hour)\n",
    "    result['pickup_latitude'] = tft.scale_to_0_1(pickup_latitude)\n",
    "    result['pickup_longitude'] = tft.scale_to_0_1(pickup_longitude)\n",
    "    result['dropoff_latitude'] = tft.scale_to_0_1(dropoff_latitude)\n",
    "    result['dropoff_longitude'] = tft.scale_to_0_1(dropoff_longitude)\n",
    "    result['passengers'] = tf.cast(passengers, tf.float32)\n",
    "    # arbitrary TF func\n",
    "    result['key'] = tf.as_string(tf.ones_like(passengers))\n",
    "    ## engineered features\n",
    "    lat_diff = pickup_latitude - dropoff_latitude\n",
    "    long_diff = pickup_longitude - dropoff_longitude\n",
    "    result['lat_diff'] = tft.scale_to_0_1(lat_diff)\n",
    "    result['long_diff'] = tft.scale_to_0_1(long_diff)\n",
    "    dist = tf.sqrt(lat_diff ** 2 + long_diff ** 2)\n",
    "    result['euclidean'] = tft.scale_to_0_1(dist)\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess(in_test_mode):\n",
    "    \"\"\" setup preprocessing pipeline. Arg boolean to run locally or in Dataflow\"\"\"\n",
    "    import os\n",
    "    import os.path\n",
    "    import tempfile\n",
    "    from apache_beam.io import tfrecordio\n",
    "    from tensorflow_transform.coders import example_proto_coder\n",
    "    from tensorflow_transform.tf_metadata import dataset_metadata, dataset_schema\n",
    "    from tensorflow_transform.beam import tft_beam_io\n",
    "    from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "    \n",
    "    dt = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "    job_name = f'preprocess-taxi-features-{dt}'\n",
    "    OUTPUT_DIR = \"./preproc_tft\" if in_test_mode else f\"gs://{bucket}/taxifare/preproc_tft/\"\n",
    "    EVERY_N = 10000 if in_test_mode else 100000\n",
    "    if in_test_mode:\n",
    "        import shutil\n",
    "        print(\"Launching local job...\")\n",
    "        shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "    else:\n",
    "        print(f\"Launching dataflow job {job_name}\")\n",
    "        import subprocess\n",
    "#         subprocess.call(f\"gsutil rm -r {OUTPUT_DIR}\")\n",
    "    \n",
    "    options = {\n",
    "        'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': project,\n",
    "        'num_workers': 1,\n",
    "        'max_num_workers': 1,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True,\n",
    "        'direct_num_workers': 1,\n",
    "        'extra_packages': ['tensorflow-transform-0.15.0.tar.gz']\n",
    "    }\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    runner = \"DirectRunner\" if in_test_mode else \"DataflowRunner\"\n",
    "    \n",
    "    raw_data_schema = {\n",
    "        colname: dataset_schema.ColumnSchema(tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "            for colname in ['dayofweek','key']\n",
    "    }\n",
    "    raw_data_schema.update({\n",
    "        colname: dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "            for colname in ['fare_amount','pickuplon','pickuplat','dropofflon','dropofflat']\n",
    "    })\n",
    "    raw_data_schema.update({\n",
    "        colname: dataset_schema.ColumnSchema(tf.int64, [], dataset_schema.FixedColumnRepresentation())\n",
    "            for colname in ['hourofday', 'passengers']\n",
    "    })\n",
    "    \n",
    "    raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
    "    \n",
    "    with beam.Pipeline(runner, options=opts) as p:\n",
    "        dir_tmp = os.path.join(OUTPUT_DIR, 'tmp')\n",
    "        dir_rawdata_md = os.path.join(OUTPUT_DIR, 'metadata', 'rawdata_metadata')\n",
    "        dir_train = os.path.join(OUTPUT_DIR, 'train')\n",
    "        dir_eval = os.path.join(OUTPUT_DIR, 'eval')\n",
    "        dir_md = os.path.join(OUTPUT_DIR, 'metadata')\n",
    "        \n",
    "        with beam_impl.Context(temp_dir=dir_tmp):\n",
    "            raw_data_metadata | 'Write Input Metadata' >> tft_beam_io.WriteMetadata(dir_rawdata_md, pipeline=p)\n",
    "            \n",
    "            raw_data = (p\n",
    "                | \"Read Training from BQ\" >> beam.io.Read(beam.io.BigQuerySource(query=create_query(1, EVERY_N), use_standard_sql=True))\n",
    "                | \"Train Filter\" >> beam.Filter(is_valid)\n",
    "            )\n",
    "            raw_dataset = (raw_data, raw_data_metadata)\n",
    "            \n",
    "            raw_test_data = (p\n",
    "                 | \"Read Eval\" >> beam.io.Read(beam.io.BigQuerySource(query=create_query(2, EVERY_N), use_standard_sql=True))\n",
    "                 | \"Eval Filter\" >> beam.Filter(is_valid)\n",
    "            )\n",
    "            raw_test_dataset = (raw_test_data, raw_data_metadata)\n",
    "            \n",
    "            transformed_dataset, transform_fn = (raw_dataset | \"Analyse and Transform Train\" >> beam_impl.AnalyzeAndTransformDataset(preprocess_tft))\n",
    "            transformed_data, transformed_metadata = transformed_dataset\n",
    "            \n",
    "            transformed_test_dataset = (raw_test_dataset, transform_fn) | \"Transform Test\" >> beam_impl.TransformDataset()\n",
    "            transformed_test_data, _ = transformed_test_dataset\n",
    "            \n",
    "            transformed_data | 'Write Train Data' >> tfrecordio.WriteToTFRecord(dir_train,\n",
    "                                                                                file_name_suffix='.gz',\n",
    "                                                                                coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            transformed_test_data | \"Write Test Data\" >> tfrecordio.WriteToTFRecord(dir_eval,\n",
    "                                                                               file_name_suffix='.gz',\n",
    "                                                                               coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            \n",
    "            # save transformation function to disk for use at serving time\n",
    "            transform_fn | \"Write Transform Function\" >> transform_fn_io.WriteTransformFn(dir_md)\n",
    "\n",
    "\n",
    "preprocess(in_test_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/\n",
      "gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/eval-00000-of-00003.gz\n",
      "gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/eval-00001-of-00003.gz\n",
      "gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/eval-00002-of-00003.gz\n",
      "gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/train-00000-of-00003.gz\n",
      "gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/train-00001-of-00003.gz\n",
      "gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/train-00002-of-00003.gz\n",
      "gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/metadata/\n",
      "gs://qwiklabs-gcp-03-cc939e8b47a1/taxifare/preproc_tft/tmp/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/taxifare/preproc_tft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'day': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:0' shape=() dtype=int64>, 'dropoff_latitude': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:1' shape=() dtype=float32>, 'dropoff_longitude': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:2' shape=() dtype=float32>, 'fare': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:3' shape=() dtype=float32>, 'hour': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:4' shape=() dtype=int64>, 'passengers': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:5' shape=() dtype=float32>, 'pickup_latitude': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:6' shape=() dtype=float32>, 'pickup_longitude': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:7' shape=() dtype=float32>}\n",
      "{'day': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:0' shape=() dtype=int64>, 'dropoff_latitude': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:1' shape=() dtype=float32>, 'dropoff_longitude': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:2' shape=() dtype=float32>, 'fare': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:3' shape=() dtype=float32>, 'hour': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:4' shape=() dtype=int64>, 'passengers': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:5' shape=() dtype=float32>, 'pickup_latitude': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:6' shape=() dtype=float32>, 'pickup_longitude': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:7' shape=() dtype=float32>}\n",
      "{'day': <tf.Tensor 'dayofweek:0' shape=(None,) dtype=int64>, 'hour': <tf.Tensor 'hourofday:0' shape=(None,) dtype=int64>, 'pickup_longitude': <tf.Tensor 'pickuplon:0' shape=(None,) dtype=float32>, 'pickup_latitude': <tf.Tensor 'pickuplat:0' shape=(None,) dtype=float32>, 'dropoff_longitude': <tf.Tensor 'dropofflon:0' shape=(None,) dtype=float32>, 'dropoff_latitude': <tf.Tensor 'dropofflat:0' shape=(None,) dtype=float32>, 'passengers': <tf.Tensor 'passengers:0' shape=(None,) dtype=float32>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './taxi_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/adagrad.py:103: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2020-05-03 12:21:00.823686: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2020-05-03 12:21:00.824119: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f8948d89e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-05-03 12:21:00.824157: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-05-03 12:21:00.824318: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 19.018723, step = 0\n",
      "INFO:tensorflow:global_step/sec: 132.444\n",
      "INFO:tensorflow:loss = 1.0650085, step = 100 (0.756 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.343\n",
      "INFO:tensorflow:loss = 14.281572, step = 200 (0.471 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into ./taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-05-03T12:21:04Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./taxi_trained/model.ckpt-300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [5/50]\n",
      "INFO:tensorflow:Evaluation [10/50]\n",
      "INFO:tensorflow:Evaluation [15/50]\n",
      "INFO:tensorflow:Evaluation [20/50]\n",
      "INFO:tensorflow:Evaluation [25/50]\n",
      "INFO:tensorflow:Evaluation [30/50]\n",
      "INFO:tensorflow:Evaluation [35/50]\n",
      "INFO:tensorflow:Evaluation [40/50]\n",
      "INFO:tensorflow:Evaluation [45/50]\n",
      "INFO:tensorflow:Evaluation [50/50]\n",
      "INFO:tensorflow:Inference Time : 1.10449s\n",
      "INFO:tensorflow:Finished evaluation at 2020-05-03-12:21:05\n",
      "INFO:tensorflow:Saving dict for global step 300: average_loss = 358.51077, global_step = 300, label/mean = 25.223438, loss = 358.51077, prediction/mean = 12.772222\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 300: ./taxi_trained/model.ckpt-300\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'day': <tf.Tensor 'dayofweek:0' shape=(None,) dtype=int64>, 'hour': <tf.Tensor 'hourofday:0' shape=(None,) dtype=int64>, 'pickup_longitude': <tf.Tensor 'dropofflon:0' shape=(None,) dtype=float32>, 'pickup_latitude': <tf.Tensor 'dropofflat:0' shape=(None,) dtype=float32>, 'dropoff_longitude': <tf.Tensor 'dropofflon:0' shape=(None,) dtype=float32>, 'dropoff_latitude': <tf.Tensor 'dropofflat:0' shape=(None,) dtype=float32>, 'passengers': <tf.Tensor 'passengers:0' shape=(None,) dtype=float32>, 'lat_diff': <tf.Tensor 'dropofflat:0' shape=(None,) dtype=float32>, 'long_diff': <tf.Tensor 'dropofflon:0' shape=(None,) dtype=float32>, 'euclidean': <tf.Tensor 'Sqrt:0' shape=(None,) dtype=float32>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'day': <tf.Tensor 'dayofweek:0' shape=(None,) dtype=int64>, 'hour': <tf.Tensor 'hourofday:0' shape=(None,) dtype=int64>, 'pickup_longitude': <tf.Tensor 'dropofflon:0' shape=(None,) dtype=float32>, 'pickup_latitude': <tf.Tensor 'dropofflat:0' shape=(None,) dtype=float32>, 'dropoff_longitude': <tf.Tensor 'dropofflon:0' shape=(None,) dtype=float32>, 'dropoff_latitude': <tf.Tensor 'dropofflat:0' shape=(None,) dtype=float32>, 'passengers': <tf.Tensor 'passengers:0' shape=(None,) dtype=float32>, 'lat_diff': <tf.Tensor 'dropofflat:0' shape=(None,) dtype=float32>, 'long_diff': <tf.Tensor 'dropofflon:0' shape=(None,) dtype=float32>, 'euclidean': <tf.Tensor 'Sqrt:0' shape=(None,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from ./taxi_trained/model.ckpt-300\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./taxi_trained/export/exporter/temp-b'1588508465'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 89.539246.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m tft_trainer.task \\\n",
    "    --train_data_path=\"gs://${BUCKET}/taxifare/preproc_tft/train*\" \\\n",
    "    --eval_data_path=\"gs://${BUCKET}/taxifare/preproc_tft/eval*\" \\\n",
    "    --output_dir=\"./taxi_trained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1588508465\n"
     ]
    }
   ],
   "source": [
    "!ls $PWD/taxi_trained/export/exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/test.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/test.json\n",
    "{\"day\":0, \"hour\":17, \"pickup_longitude\": -73.885262, \"pickup_latitude\": 40.773008, \"dropoff_longitude\": -73.987232, \"dropoff_latitude\": 40.732403, \"passengers\": 2.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo find \"/usr/lib/google-cloud-sdk/lib/googlecloudsdk/command_lib/ml_engine\" -name '*.pyc' -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS\n",
      "[69.15485382080078]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If the signature defined in the model is not serving_default then you must specify it via --signature-name flag, otherwise the command may fail.\n",
      "WARNING: WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2020-05-03 12:26:14.034921: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2020-05-03 12:26:14.035297: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555ffed7d080 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-05-03 12:26:14.035333: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-05-03 12:26:14.035541: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:236: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:236: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "model_dir=$(ls $PWD/taxi_trained/export/exporter/)\n",
    "gcloud ai-platform local predict \\\n",
    "    --model-dir=\"./taxi_trained/export/exporter/${model_dir}\" \\\n",
    "    --json-instances=\"/tmp/test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"feateng\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/taxifare/ch4/taxi_trained/export/exporter | tail -1)\n",
    "echo \"Run these commands one-by-one (the very first time, you'll create a model and then create a version)\"\n",
    "#gcloud ai-platform versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ai-platform delete ${MODEL_NAME}\n",
    "gcloud ai-platform models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ai-platform versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version $TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gcloud ai-platform predict --model=feateng --version=v1 --json-instances=/tmp/test.json\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}